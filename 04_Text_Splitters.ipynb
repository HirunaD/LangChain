{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkhhrB4N2dV4dxdyGcs4hy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HirunaD/LangChain/blob/main/04_Text_Splitters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Splitters in LangChain**"
      ],
      "metadata": {
        "id": "WRxlvKI0JTJr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTFsO1EsI0xC",
        "outputId": "dc53f5df-d4bb-4ef3-b298-a444ee880860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary packages\n",
        "!pip install langchain -qU\n",
        "!pip install langchain-community -qU\n",
        "!pip install unstructured -qU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "# Initialize the DirectoryLoader with the path to the directory for text files\n",
        "loader = DirectoryLoader(\"/content/data\", glob=\"**/*.txt\")\n",
        "\n",
        "# Load the text data from the directory\n",
        "dataset = loader.load()\n",
        "\n",
        "for data in dataset:\n",
        "  print(\"------------------------\")\n",
        "  print(data.page_content)\n",
        "  print(data.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3Dxa6aHJ7uq",
        "outputId": "8bd1c898-609b-456a-fcbb-d949eadb8179"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------\n",
            "CodePRO LK is an educational platform offering a wide range of online courses primarily focused on programming and machine learning. All courses are provided for free and are delivered in Sinhala, catering specifically to the Sri Lankan audience. The platform includes various learning materials such as videos, quizzes, and assignments to enhance the learning experience​​.\n",
            "\n",
            "Some of the featured courses on CodePRO LK include \"Python GUI – Tkinter,\" \"Machine Learning Project – Sentiment Analysis,\" and \"Data Structures and Algorithms.\" These courses range from beginner to intermediate levels, making them accessible to a broad spectrum of learners​.\n",
            "\n",
            "In addition to the courses, CodePRO LK also offers projects and resources to support learners in applying their knowledge practically. The platform emphasizes providing high-quality education to empower the next generation of Sri Lankans in the tech field​​.\n",
            "{'source': '/content/data/text2.txt'}\n",
            "------------------------\n",
            "LangChain is a robust framework designed to facilitate the creation of applications leveraging large language models (LLMs). By providing a suite of tools and utilities, LangChain simplifies the process of integrating LLMs into various applications, enabling developers to build sophisticated AI-powered solutions with greater ease. Its capabilities span from natural language understanding and generation to complex data processing tasks, making it a versatile choice for developing chatbots, virtual assistants, and other AI-driven applications.\n",
            "\n",
            "One of the core features of LangChain is its modular architecture, which allows developers to customize and extend the framework according to their specific needs. This modularity ensures that different components, such as model training, data preprocessing, and inference, can be independently developed and optimized. Additionally, LangChain supports seamless integration with popular machine learning libraries and frameworks, providing a comprehensive ecosystem for end-to-end development of language-based applications.\n",
            "\n",
            "LangChain also emphasizes the importance of scalability and performance. It includes optimized pipelines and efficient data handling mechanisms that can process large datasets and deliver real-time responses. This focus on performance makes it suitable for deploying applications in production environments where speed and reliability are critical.\n",
            "\n",
            "Moreover, LangChain is designed with usability in mind. It offers extensive documentation, tutorials, and community support to help developers of all skill levels get started. Whether you are an experienced machine learning engineer or a beginner in the field, LangChain provides the resources and tools needed to build effective language-driven applications.\n",
            "{'source': '/content/data/text1.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the token count of each document in the dataset using a character level tokenizer\n",
        "token_counts = [len(data.page_content) for data in dataset]\n",
        "\n",
        "print(token_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCgY37IHK2Nq",
        "outputId": "184e0b69-12bc-4f52-9636-4ea5ac37aa86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[912, 1784]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Token Count Using tiktoken**"
      ],
      "metadata": {
        "id": "LM3NVWKzI9k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the tiktoken package for tokenization\n",
        "!pip install tiktoken -qU"
      ],
      "metadata": {
        "id": "XyqFzm5MK_mB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer_model = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
        "print(tokenizer_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ayXRNE6LENx",
        "outputId": "704aa55b-90c5-4db7-cbca-4819bb1216d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Encoding 'cl100k_base'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the encoding for the tokenizer\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "# Create a function to calculate the length of text in tokens using tiktoken\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(text)\n",
        "    token_length = len(tokens)\n",
        "    return token_length"
      ],
      "metadata": {
        "id": "Cj_HLgtDLJV9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the token count of each document in the dataset using tiktoken\n",
        "token_counts = [tiktoken_len(data.page_content) for data in dataset]\n",
        "\n",
        "print(token_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmB0CUblLZBl",
        "outputId": "caef3f54-e0c2-4ff6-aa9f-ac4b2a750a4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[168, 286]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking the Text Using RecursiveCharacterTextSplitter**"
      ],
      "metadata": {
        "id": "dQoPAzPBI7OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize the text splitter with a chunk size of 150 and no overlap, using character length function\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        "    separators=['\\n\\n', '\\n', ' ', '']\n",
        ")"
      ],
      "metadata": {
        "id": "h4JLpUH5Md1V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text of the second document in the dataset into chunks\n",
        "chunks = text_splitter.split_text(dataset[1].page_content)\n",
        "\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KJBoVFZMwOG",
        "outputId": "23f142e2-5f5b-4c87-adaf-cf0053240658"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[0])\n",
        "print(len(chunks[0]))\n",
        "print(chunks[1])\n",
        "print(len(chunks[1]))\n",
        "print(chunks[2])\n",
        "print(len(chunks[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aazQxDFNM8Ua",
        "outputId": "20af0f07-8df3-42f7-beff-c6af137c7a07"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is a robust framework designed to facilitate the creation of applications leveraging large language models (LLMs). By providing a suite of\n",
            "148\n",
            "tools and utilities, LangChain simplifies the process of integrating LLMs into various applications, enabling developers to build sophisticated\n",
            "143\n",
            "AI-powered solutions with greater ease. Its capabilities span from natural language understanding and generation to complex data processing tasks,\n",
            "146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinitialize the text splitter with a chunk size of 150 and an overlap of 20, using character length function\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    separators=['\\n\\n', '\\n', ' ', '']\n",
        ")"
      ],
      "metadata": {
        "id": "BZ0GItX7Nctl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text of the second document in the dataset into chunks\n",
        "chunks = text_splitter.split_text(dataset[1].page_content)\n",
        "\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuoRYjEANgNu",
        "outputId": "9a15d48b-62c3-4d93-c492-f48ca5891d83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[0])\n",
        "print(len(chunks[0]))\n",
        "print(chunks[1])\n",
        "print(len(chunks[1]))\n",
        "print(chunks[2])\n",
        "print(len(chunks[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ancFyM8cNk4N",
        "outputId": "67bd2ea5-c3a3-4d74-81d4-f370270d4309"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is a robust framework designed to facilitate the creation of applications leveraging large language models (LLMs). By providing a suite of\n",
            "148\n",
            "a suite of tools and utilities, LangChain simplifies the process of integrating LLMs into various applications, enabling developers to build\n",
            "140\n",
            "developers to build sophisticated AI-powered solutions with greater ease. Its capabilities span from natural language understanding and generation to\n",
            "149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking the Text Using tiktoken Length Function**"
      ],
      "metadata": {
        "id": "1cfNihSsObjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinitialize the text splitter with a chunk size of 150 and an overlap of 20, using tiktoken length function\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,\n",
        "    chunk_overlap=20,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=['\\n\\n', '\\n', ' ', '']\n",
        ")"
      ],
      "metadata": {
        "id": "U_8uR4e_OZC4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text of the second document in the dataset into chunks\n",
        "chunks = text_splitter.split_text(dataset[1].page_content)\n",
        "\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY1CgQNnOkra",
        "outputId": "064a3f18-4419-46c1-f6a3-311b90b88fb8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[0])\n",
        "print(tiktoken_len(chunks[0]))\n",
        "print(chunks[1])\n",
        "print(tiktoken_len(chunks[1]))\n",
        "print(chunks[2])\n",
        "print(tiktoken_len(chunks[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGkcuSsHOtMM",
        "outputId": "26d6c134-e84c-4bda-f5de-ddcabd405735"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is a robust framework designed to facilitate the creation of applications leveraging large language models (LLMs). By providing a suite of tools and utilities, LangChain simplifies the process of integrating LLMs into various applications, enabling developers to build sophisticated AI-powered solutions with greater ease. Its capabilities span from natural language understanding and generation to complex data processing tasks, making it a versatile choice for developing chatbots, virtual assistants, and other AI-driven applications.\n",
            "91\n",
            "One of the core features of LangChain is its modular architecture, which allows developers to customize and extend the framework according to their specific needs. This modularity ensures that different components, such as model training, data preprocessing, and inference, can be independently developed and optimized. Additionally, LangChain supports seamless integration with popular machine learning libraries and frameworks, providing a comprehensive ecosystem for end-to-end development of language-based applications.\n",
            "\n",
            "LangChain also emphasizes the importance of scalability and performance. It includes optimized pipelines and efficient data handling mechanisms that can process large datasets and deliver real-time responses. This focus on performance makes it suitable for deploying applications in production environments where speed and reliability are critical.\n",
            "134\n",
            "Moreover, LangChain is designed with usability in mind. It offers extensive documentation, tutorials, and community support to help developers of all skill levels get started. Whether you are an experienced machine learning engineer or a beginner in the field, LangChain provides the resources and tools needed to build effective language-driven applications.\n",
            "61\n"
          ]
        }
      ]
    }
  ]
}